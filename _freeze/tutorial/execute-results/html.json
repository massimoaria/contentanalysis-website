{
  "hash": "d3ab6ded1fdb89e9fd7de8db7e95d800",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Complete Tutorial\"\nsubtitle: \"End-to-End Workflow for Scientific Content Analysis\"\n---\n\n## Introduction\n\nThis tutorial provides a complete workflow for analyzing scientific papers using the contentanalysis package. We'll work through a real example, from PDF import to final visualizations and reporting.\n\n## Setup\n\n### Install Required Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install contentanalysis\ndevtools::install_github(\"massimoaria/contentanalysis\")\n\n# Install supporting packages\ninstall.packages(c(\"dplyr\", \"ggplot2\", \"tidyr\", \"knitr\"))\n\n# Load libraries\nlibrary(contentanalysis)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n```\n:::\n\n\n::: {.callout-tip}\n## Optional: Setup AI-Enhanced Features ðŸ†•\n\nFor improved PDF extraction with complex layouts:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get API key from https://aistudio.google.com/apikey\nSys.setenv(GEMINI_API_KEY = \"your-api-key-here\")\n\n# Or add to .Renviron file:\n# GEMINI_API_KEY=your-api-key-here\n```\n:::\n\n:::\n\n## Step 1: Obtain Sample Paper\n\nWe'll use an open-access paper on Machine Learning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download example paper\npaper_url <- \"https://raw.githubusercontent.com/massimoaria/contentanalysis/master/inst/examples/example_paper.pdf\"\ndownload.file(paper_url, destfile = \"example_paper.pdf\", mode = \"wb\")\n\n# Verify download\nfile.exists(\"example_paper.pdf\")\n```\n:::\n\n\n::: {.callout-note}\n## Using Your Own Papers\n\nReplace the URL with your own PDF file path. Ensure the PDF is text-based (not a scanned image).\n:::\n\n## Step 2: Import and Inspect PDF\n\n### Import with Section Detection\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import PDF with automatic section detection\ndoc <- pdf2txt_auto(\n  \"example_paper.pdf\",\n  n_columns = 2,          # Two-column layout\n  sections = TRUE         # Detect sections\n)\n\n# Check detected sections\ncat(\"Detected sections:\\n\")\nprint(names(doc))\n\n# Preview Abstract\ncat(\"\\n=== Abstract Preview ===\\n\")\ncat(substr(doc$Abstract, 1, 500), \"...\\n\")\n```\n:::\n\n\n### Verify Section Quality\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check section word counts\nsection_lengths <- sapply(doc[names(doc) != \"Full_text\"], function(x) {\n  length(strsplit(x, \"\\\\s+\")[[1]])\n})\n\nsection_df <- data.frame(\n  section = names(section_lengths),\n  words = section_lengths\n) %>%\n  arrange(desc(words))\n\nprint(section_df)\n\n# Visualize section lengths\nggplot(section_df, aes(x = reorder(section, words), y = words, fill = section)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Word Count by Section\",\n       x = \"Section\", y = \"Number of Words\") +\n  theme_minimal()\n```\n:::\n\n\n## Step 3: Comprehensive Content Analysis\n\n### Main Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform comprehensive analysis with enhanced metadata integration\nanalysis <- analyze_scientific_content(\n  text = doc,\n  doi = \"10.1016/j.mlwa.2021.100094\",  # Paper's DOI\n  mailto = \"your@email.com\",            # Your email for CrossRef\n  window_size = 10,                     # Context window\n  remove_stopwords = TRUE,              # Remove common words\n  ngram_range = c(1, 3),               # Unigrams to trigrams\n  use_sections_for_citations = TRUE\n)\n\n# View summary\nprint(analysis$summary)\n```\n:::\n\n\n::: {.callout-note}\n## ðŸ†• Enhanced Features\n\nThe analysis now includes:\n\n- **Dual metadata integration**: Automatically retrieves references from both CrossRef and OpenAlex\n- **Improved citation matching**: Better handling of numeric citations (`[1]`, `[1-3]`) and author-year formats\n- **Enhanced confidence scoring**: More granular assessment of match quality\n- **Better author name handling**: Resolves variants like \"Smith, J.\" vs \"Smith, John\"\n:::\n\n### Interpret Summary Statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract key metrics\ntotal_words <- analysis$summary$total_words\ncitations <- analysis$summary$citations_extracted\ndensity <- analysis$summary$citation_density\ndiversity <- analysis$summary$lexical_diversity\n\ncat(\"Document Statistics:\\n\")\ncat(\"===================\\n\")\ncat(sprintf(\"Total words: %d\\n\", total_words))\ncat(sprintf(\"Citations: %d\\n\", citations))\ncat(sprintf(\"Citation density: %.2f per 1000 words\\n\", density))\ncat(sprintf(\"Lexical diversity: %.3f\\n\", diversity))\n\n# Assess citation intensity\nif (density < 5) {\n  cat(\"\\nâ†’ Low citation density (typical for theoretical papers)\\n\")\n} else if (density < 15) {\n  cat(\"\\nâ†’ Moderate citation density (standard empirical paper)\\n\")\n} else {\n  cat(\"\\nâ†’ High citation density (review paper or methods paper)\\n\")\n}\n```\n:::\n\n\n## Step 4: Citation Analysis\n\n### Extract and Explore Citations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# View first citations\nhead(analysis$citations, 10)\n\n# Citation types\ncitation_summary <- analysis$citations %>%\n  group_by(citation_type) %>%\n  summarise(count = n(), .groups = \"drop\") %>%\n  mutate(percentage = round(count / sum(count) * 100, 1))\n\nprint(citation_summary)\n\n# Visualize\nggplot(citation_summary, aes(x = citation_type, y = count, fill = citation_type)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(aes(label = paste0(count, \" (\", percentage, \"%)\")), \n            vjust = -0.5) +\n  labs(title = \"Citation Types\",\n       x = \"Type\", y = \"Count\") +\n  theme_minimal()\n```\n:::\n\n\n### Citations by Section\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Citation distribution across sections\nsection_citations <- analysis$citations %>%\n  count(section, sort = TRUE)\n\nprint(section_citations)\n\n# Visualize\nggplot(section_citations, aes(x = reorder(section, n), y = n, fill = section)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Citations by Section\",\n       x = \"Section\", y = \"Number of Citations\") +\n  theme_minimal()\n```\n:::\n\n\n### Most Cited References\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Top 10 most cited references\ntop_cited <- analysis$citation_references_mapping %>%\n  count(ref_full_text, sort = TRUE) %>%\n  head(10) %>%\n  mutate(ref_short = substr(ref_full_text, 1, 60))\n\nprint(top_cited)\n\n# Visualize\nggplot(top_cited, aes(x = reorder(ref_short, n), y = n)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Most Cited References\",\n       x = NULL, y = \"Citation Count\") +\n  theme_minimal()\n```\n:::\n\n\n### Citation Contexts\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Examine citation contexts\ncontexts_sample <- analysis$citation_contexts %>%\n  select(citation_text_clean, section, words_before, words_after) %>%\n  head(5)\n\nprint(contexts_sample)\n\n# Find method citations\nmethod_citations <- analysis$citation_contexts %>%\n  filter(grepl(\"method|approach|algorithm|technique\", \n               paste(words_before, words_after), \n               ignore.case = TRUE)) %>%\n  select(citation_text_clean, section, words_before, words_after)\n\ncat(\"\\nMethod-related citations found:\", nrow(method_citations), \"\\n\")\nhead(method_citations)\n```\n:::\n\n\n## Step 5: Network Visualization\n\n### Create Citation Network\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create interactive network\nnetwork <- create_citation_network(\n  citation_analysis_results = analysis,\n  max_distance = 800,\n  min_connections = 2,\n  show_labels = TRUE\n)\n\n# Display network\nnetwork\n```\n:::\n\n\n### Analyze Network Statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get network statistics\nstats <- attr(network, \"stats\")\n\ncat(\"Network Statistics:\\n\")\ncat(\"===================\\n\")\ncat(\"Nodes:\", stats$n_nodes, \"\\n\")\ncat(\"Edges:\", stats$n_edges, \"\\n\")\ncat(\"Avg distance:\", round(stats$avg_distance), \"characters\\n\")\ncat(\"Max distance:\", stats$max_distance, \"characters\\n\")\n\n# Network density\ndensity <- stats$n_edges / (stats$n_nodes * (stats$n_nodes - 1) / 2)\ncat(\"Network density:\", round(density, 3), \"\\n\")\n\n# Section distribution\nprint(stats$section_distribution)\n\n# Hub citations\nhub_threshold <- quantile(stats$section_distribution$n, 0.75)\nhubs <- stats$section_distribution %>%\n  filter(n >= hub_threshold) %>%\n  arrange(desc(n))\n\ncat(\"\\nHub citations (top 25%):\\n\")\nprint(hubs)\n```\n:::\n\n\n### Export Network\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(htmlwidgets)\n\n# Save as standalone HTML\nsaveWidget(network, \n           \"citation_network.html\",\n           selfcontained = TRUE,\n           title = \"Citation Network\")\n\ncat(\"Network saved to: citation_network.html\\n\")\n```\n:::\n\n\n## Step 6: Text Analysis\n\n### Word Frequency Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Top 30 words\ntop_words <- head(analysis$word_frequencies, 30)\nprint(top_words)\n\n# Visualize top 20\ntop_20 <- head(analysis$word_frequencies, 20)\n\nggplot(top_20, aes(x = reorder(word, frequency), y = frequency)) +\n  geom_col(fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Top 20 Most Frequent Words\",\n       x = \"Word\", y = \"Frequency\") +\n  theme_minimal()\n```\n:::\n\n\n### N-gram Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most common bigrams\ntop_bigrams <- head(analysis$ngrams$`2gram`, 15)\nprint(top_bigrams)\n\n# Most common trigrams\ntop_trigrams <- head(analysis$ngrams$`3gram`, 10)\nprint(top_trigrams)\n\n# Visualize bigrams\nggplot(top_bigrams, aes(x = reorder(ngram, frequency), y = frequency)) +\n  geom_col(fill = \"coral\") +\n  coord_flip() +\n  labs(title = \"Top 15 Bigrams\",\n       x = \"Bigram\", y = \"Frequency\") +\n  theme_minimal()\n```\n:::\n\n\n### Word Distribution Tracking\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define key terms to track\nkey_terms <- c(\"machine learning\", \"random forest\", \"accuracy\", \n               \"classification\", \"model\")\n\n# Calculate distribution\ndist <- calculate_word_distribution(\n  text = doc,\n  selected_words = key_terms,\n  use_sections = TRUE,\n  normalize = TRUE\n)\n\n# View results\nprint(dist)\n\n# Interactive visualization\nplot_word_distribution(\n  dist,\n  plot_type = \"line\",\n  show_points = TRUE,\n  smooth = TRUE\n)\n```\n:::\n\n\n## Step 7: Readability Assessment\n\n### Overall Readability\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate readability for full text\nreadability <- calculate_readability_indices(\n  doc$Full_text,\n  detailed = TRUE\n)\n\nprint(readability)\n\n# Interpret\ncat(\"\\nInterpretation:\\n\")\ncat(\"Flesch Reading Ease:\", readability$flesch_reading_ease, \"\\n\")\nif (readability$flesch_reading_ease < 30) {\n  cat(\"â†’ Very difficult (graduate level)\\n\")\n} else if (readability$flesch_reading_ease < 50) {\n  cat(\"â†’ Difficult (college level)\\n\")\n} else {\n  cat(\"â†’ Fairly difficult (high school to college)\\n\")\n}\n\ncat(\"\\nGrade Level:\", round(readability$flesch_kincaid_grade, 1), \"\\n\")\n```\n:::\n\n\n### Compare Sections\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate for each section\nsections <- c(\"Abstract\", \"Introduction\", \"Methods\", \"Results\", \"Discussion\")\nsection_readability <- data.frame()\n\nfor (section in sections) {\n  if (section %in% names(doc)) {\n    metrics <- calculate_readability_indices(doc[[section]], detailed = FALSE)\n    metrics$section <- section\n    section_readability <- rbind(section_readability, metrics)\n  }\n}\n\nprint(section_readability)\n\n# Visualize\nggplot(section_readability, \n       aes(x = reorder(section, flesch_reading_ease), \n           y = flesch_reading_ease, fill = section)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Readability by Section\",\n       subtitle = \"Higher scores = easier to read\",\n       x = \"Section\", y = \"Flesch Reading Ease\") +\n  theme_minimal()\n```\n:::\n\n\n## Step 8: Comprehensive Reporting\n\n### Create Summary Report\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compile comprehensive report\nreport <- list(\n  document_info = list(\n    doi = \"10.1016/j.mlwa.2021.100094\",\n    total_words = analysis$summary$total_words,\n    sections = names(doc)[names(doc) != \"Full_text\"]\n  ),\n  \n  citation_metrics = list(\n    total_citations = analysis$summary$citations_extracted,\n    narrative = analysis$summary$narrative_citations,\n    parenthetical = analysis$summary$parenthetical_citations,\n    matched = analysis$summary$references_matched,\n    density = analysis$summary$citation_density\n  ),\n  \n  text_metrics = list(\n    lexical_diversity = analysis$summary$lexical_diversity,\n    top_10_words = head(analysis$word_frequencies$word, 10),\n    top_10_bigrams = head(analysis$ngrams$`2gram`$ngram, 10)\n  ),\n  \n  readability = list(\n    flesch_reading_ease = readability$flesch_reading_ease,\n    grade_level = readability$flesch_kincaid_grade,\n    gunning_fog = readability$gunning_fog\n  ),\n  \n  network_stats = list(\n    nodes = stats$n_nodes,\n    edges = stats$n_edges,\n    density = stats$n_edges / (stats$n_nodes * (stats$n_nodes - 1) / 2)\n  )\n)\n\n# Print report\ncat(\"COMPREHENSIVE ANALYSIS REPORT\\n\")\ncat(\"=============================\\n\\n\")\n\ncat(\"DOCUMENT INFORMATION\\n\")\ncat(\"DOI:\", report$document_info$doi, \"\\n\")\ncat(\"Total words:\", report$document_info$total_words, \"\\n\")\ncat(\"Sections:\", paste(report$document_info$sections, collapse = \", \"), \"\\n\\n\")\n\ncat(\"CITATION METRICS\\n\")\ncat(\"Total citations:\", report$citation_metrics$total_citations, \"\\n\")\ncat(\"Citation density:\", round(report$citation_metrics$density, 2), \"per 1000 words\\n\")\ncat(\"Match rate:\", round(report$citation_metrics$matched / report$citation_metrics$total_citations * 100, 1), \"%\\n\\n\")\n\ncat(\"TEXT METRICS\\n\")\ncat(\"Lexical diversity:\", round(report$text_metrics$lexical_diversity, 3), \"\\n\")\ncat(\"Top words:\", paste(head(report$text_metrics$top_10_words, 5), collapse = \", \"), \"\\n\\n\")\n\ncat(\"READABILITY\\n\")\ncat(\"Reading ease:\", round(report$readability$flesch_reading_ease, 1), \"\\n\")\ncat(\"Grade level:\", round(report$readability$grade_level, 1), \"\\n\\n\")\n\ncat(\"NETWORK STATISTICS\\n\")\ncat(\"Citation nodes:\", report$network_stats$nodes, \"\\n\")\ncat(\"Connections:\", report$network_stats$edges, \"\\n\")\ncat(\"Density:\", round(report$network_stats$density, 3), \"\\n\")\n```\n:::\n\n\n### Export All Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create output directory\ndir.create(\"analysis_output\", showWarnings = FALSE)\n\n# 1. Citations\nwrite.csv(analysis$citations, \n          \"analysis_output/citations.csv\", \n          row.names = FALSE)\n\n# 2. Matched references\nwrite.csv(analysis$citation_references_mapping,\n          \"analysis_output/matched_references.csv\",\n          row.names = FALSE)\n\n# 3. Word frequencies\nwrite.csv(analysis$word_frequencies,\n          \"analysis_output/word_frequencies.csv\",\n          row.names = FALSE)\n\n# 4. Bigrams\nwrite.csv(analysis$ngrams$`2gram`,\n          \"analysis_output/bigrams.csv\",\n          row.names = FALSE)\n\n# 5. Trigrams\nwrite.csv(analysis$ngrams$`3gram`,\n          \"analysis_output/trigrams.csv\",\n          row.names = FALSE)\n\n# 6. Network statistics\nwrite.csv(stats$section_distribution,\n          \"analysis_output/network_stats.csv\",\n          row.names = FALSE)\n\n# 7. Readability by section\nwrite.csv(section_readability,\n          \"analysis_output/readability.csv\",\n          row.names = FALSE)\n\n# 8. Summary report as JSON\nlibrary(jsonlite)\nwrite_json(report, \n           \"analysis_output/summary_report.json\",\n           pretty = TRUE, \n           auto_unbox = TRUE)\n\ncat(\"All results exported to: analysis_output/\\n\")\n```\n:::\n\n\n## Step 9: Advanced Visualizations\n\n### Create Publication-Ready Figures\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\n# Figure 1: Overview\np1 <- ggplot(section_df, aes(x = reorder(section, words), y = words)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"A) Document Structure\", x = NULL, y = \"Words\") +\n  theme_minimal()\n\np2 <- ggplot(citation_summary, aes(x = citation_type, y = count, fill = citation_type)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"B) Citation Types\", x = NULL, y = \"Count\") +\n  theme_minimal()\n\n# Combine\ncombined <- p1 + p2\nprint(combined)\n\nggsave(\"analysis_output/figure1_overview.png\", \n       combined, width = 10, height = 5, dpi = 300)\n\n# Figure 2: Text analysis\np3 <- ggplot(head(top_words, 15), \n             aes(x = reorder(word, frequency), y = frequency)) +\n  geom_col(fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"A) Top Words\", x = NULL, y = \"Frequency\") +\n  theme_minimal()\n\np4 <- ggplot(section_readability, \n             aes(x = reorder(section, flesch_reading_ease), \n                 y = flesch_reading_ease)) +\n  geom_col(fill = \"coral\") +\n  coord_flip() +\n  labs(title = \"B) Readability\", x = NULL, y = \"FRE Score\") +\n  theme_minimal()\n\ncombined2 <- p3 + p4\nprint(combined2)\n\nggsave(\"analysis_output/figure2_text_analysis.png\",\n       combined2, width = 10, height = 5, dpi = 300)\n```\n:::\n\n\n## Step 10: Batch Processing\n\n### Analyze Multiple Papers\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define papers to analyze\npapers_df <- data.frame(\n  file = c(\"paper1.pdf\", \"paper2.pdf\", \"paper3.pdf\"),\n  doi = c(\"10.xxxx/1\", \"10.xxxx/2\", \"10.xxxx/3\"),\n  name = c(\"Paper A\", \"Paper B\", \"Paper C\"),\n  stringsAsFactors = FALSE\n)\n\n# Process all papers\nall_results <- list()\nall_networks <- list()\n\nfor (i in 1:nrow(papers_df)) {\n  cat(\"\\nProcessing:\", papers_df$name[i], \"\\n\")\n  \n  # Import\n  doc <- pdf2txt_auto(papers_df$file[i], n_columns = 2)\n  \n  # Analyze\n  all_results[[i]] <- analyze_scientific_content(\n    text = doc,\n    doi = papers_df$doi[i],\n    mailto = \"your@email.com\"\n  )\n  \n  # Network\n  all_networks[[i]] <- create_citation_network(\n    all_results[[i]],\n    max_distance = 800,\n    min_connections = 2\n  )\n  \n  Sys.sleep(1)  # Be polite to CrossRef API\n}\n\nnames(all_results) <- papers_df$name\nnames(all_networks) <- papers_df$name\n\n# Compare papers\ncomparison <- data.frame(\n  paper = papers_df$name,\n  words = sapply(all_results, function(r) r$summary$total_words),\n  citations = sapply(all_results, function(r) r$summary$citations_extracted),\n  density = sapply(all_results, function(r) r$summary$citation_density),\n  diversity = sapply(all_results, function(r) r$summary$lexical_diversity),\n  network_nodes = sapply(all_networks, function(n) attr(n, \"stats\")$n_nodes),\n  network_edges = sapply(all_networks, function(n) attr(n, \"stats\")$n_edges)\n)\n\nprint(comparison)\n\n# Visualize comparison\ncomparison_long <- comparison %>%\n  select(paper, citations, density, diversity) %>%\n  pivot_longer(cols = -paper, names_to = \"metric\", values_to = \"value\")\n\nggplot(comparison_long, aes(x = paper, y = value, fill = paper)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~metric, scales = \"free_y\") +\n  labs(title = \"Comparison Across Papers\",\n       x = NULL, y = \"Value\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n:::\n\n\n## Conclusion\n\nThis tutorial covered the complete workflow:\n\n1. âœ“ PDF import with section detection\n2. âœ“ Comprehensive content analysis\n3. âœ“ Citation extraction and matching\n4. âœ“ Interactive network visualization\n5. âœ“ Text analysis and n-grams\n6. âœ“ Readability assessment\n7. âœ“ Comprehensive reporting\n8. âœ“ Data export\n9. âœ“ Publication-ready figures\n10. âœ“ Batch processing\n\n## Next Steps\n\n- Explore [Reference Documentation](reference/) for detailed function information\n- Try the analysis on your own papers\n- Customize visualizations for your needs\n- Integrate into your research workflow\n\n## Resources\n\n- [GitHub Repository](https://github.com/massimoaria/contentanalysis)\n- [Issue Tracker](https://github.com/massimoaria/contentanalysis/issues)\n- [Get Started Guide](get-started.qmd)\n\n## Troubleshooting\n\n### Common Issues\n\n**PDF Import Problems**\n```r\n# Try different column settings\ndoc1 <- pdf2txt_auto(\"paper.pdf\", n_columns = 1)\ndoc2 <- pdf2txt_auto(\"paper.pdf\", n_columns = 2)\n# Compare which works better\n```\n\n**Low Citation Matching**\n```r\n# Ensure DOI and email are provided\n# Check References section was extracted\nnames(doc)  # Should include \"References\"\n```\n\n**Network Not Displaying**\n```r\n# Adjust parameters\nnetwork <- create_citation_network(\n  analysis,\n  max_distance = 1000,  # Increase\n  min_connections = 1    # Decrease\n)\n```\n\nFor more help, see the [Get Started](get-started.qmd) troubleshooting section.",
    "supporting": [
      "tutorial_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}