{
  "hash": "6179909747ab7e6aa70d6e1bcd61400c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Get Started\"\n---\n\n## Installation\n\nInstall the development version from GitHub:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install devtools if not already installed\nif (!require(\"devtools\")) install.packages(\"devtools\")\n\n# Install contentanalysis\ndevtools::install_github(\"massimoaria/contentanalysis\")\n```\n:::\n\n\n::: {.callout-tip}\n## AI-Enhanced PDF Import (Optional)\n\nFor improved text extraction from complex PDFs, you can enable AI support:\n\n1. Get a free API key from [Google AI Studio](https://aistudio.google.com/apikey)\n2. Set the environment variable:\n\n::: {.cell}\n\n```{.r .cell-code}\n# In R\nSys.setenv(GEMINI_API_KEY = \"your-api-key-here\")\n\n# Or in your .Renviron file\nGEMINI_API_KEY=your-api-key-here\n```\n:::\n\n:::\n\nLoad the package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(contentanalysis)\nlibrary(dplyr)\n```\n:::\n\n\n## Your First Analysis\n\n### Step 1: Download an Example Paper\n\nWe'll use an open-access paper on Machine Learning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download example paper\npaper_url <- \"https://raw.githubusercontent.com/massimoaria/contentanalysis/master/inst/examples/example_paper.pdf\"\ndownload.file(paper_url, destfile = \"example_paper.pdf\", mode = \"wb\")\n```\n:::\n\n\n### Step 2: Import the PDF\n\nImport with automatic section detection:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import PDF with automatic section detection\ndoc <- pdf2txt_auto(\"example_paper.pdf\", n_columns = 2)\n\n# Check detected sections\nnames(doc)\n```\n:::\n\n\nExpected output:\n```\n[1] \"Full_text\"    \"Abstract\"     \"Introduction\" \"Methods\"     \n[5] \"Results\"      \"Discussion\"   \"References\"\n```\n\n### Step 3: Analyze the Content\n\nPerform comprehensive analysis with CrossRef and OpenAlex integration:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanalysis <- analyze_scientific_content(\n  text = doc,\n  doi = \"10.1016/j.mlwa.2021.100094\",\n  mailto = \"your@email.com\",\n  window_size = 10,\n  remove_stopwords = TRUE,\n  ngram_range = c(1, 3)\n)\n```\n:::\n\n\n::: {.callout-tip}\n## Enhanced Metadata Integration ðŸ†•\n\nThe package now automatically enriches references with metadata from **both CrossRef and OpenAlex**:\n\n- **CrossRef**: Retrieves structured reference data including authors, years, journals, and DOIs\n- **OpenAlex**: Fills gaps and provides comprehensive bibliographic information\n- **Improved matching**: Enhanced algorithms for connecting citations to references with confidence scoring\n\nThis dual integration significantly improves citation-reference matching accuracy!\n:::\n\n### Step 4: Explore the Results\n\nView summary statistics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanalysis$summary\n```\n:::\n\n\nExample output:\n```\n$total_words\n[1] 5234\n\n$citations_extracted\n[1] 42\n\n$narrative_citations\n[1] 18\n\n$parenthetical_citations\n[1] 24\n\n$references_matched\n[1] 38\n\n$lexical_diversity\n[1] 0.421\n\n$citation_density\n[1] 8.03\n```\n\n## Common Workflows\n\n### Workflow 1: Citation Analysis\n\nExtract and analyze citations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# View all citations\nhead(analysis$citations)\n\n# Count by type\ntable(analysis$citations$citation_type)\n\n# Find citations in specific section\nintro_citations <- analysis$citations %>%\n  filter(section == \"Introduction\")\n\nnrow(intro_citations)\n```\n:::\n\n\n### Workflow 2: Text Analysis\n\nAnalyze word usage:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Top words\nhead(analysis$word_frequencies, 20)\n\n# Bigrams\nhead(analysis$ngrams$`2gram`, 10)\n\n# Track specific terms\nterms <- c(\"machine learning\", \"random forest\", \"accuracy\")\n\ndist <- calculate_word_distribution(\n  text = doc,\n  selected_words = terms,\n  use_sections = TRUE\n)\n\n# Visualize\nplot_word_distribution(dist, plot_type = \"line\")\n```\n:::\n\n\n### Workflow 3: Network Visualization\n\nCreate citation networks:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create network\nnetwork <- create_citation_network(\n  analysis,\n  max_distance = 800,\n  min_connections = 2,\n  show_labels = TRUE\n)\n\n# Display\nnetwork\n\n# View statistics\nstats <- attr(network, \"stats\")\nprint(stats$section_distribution)\n```\n:::\n\n\n### Workflow 4: Readability Assessment\n\nCalculate readability metrics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Full document readability\nreadability <- calculate_readability_indices(\n  doc$Full_text,\n  detailed = TRUE\n)\n\nprint(readability)\n\n# Compare sections\nsections <- c(\"Abstract\", \"Introduction\", \"Methods\", \"Discussion\")\nreadability_by_section <- lapply(sections, function(s) {\n  calculate_readability_indices(doc[[s]], detailed = FALSE)\n})\nnames(readability_by_section) <- sections\n\ndo.call(rbind, readability_by_section)\n```\n:::\n\n\n### Workflow 5: AI-Enhanced PDF Import ðŸ†•\n\nFor complex PDFs with difficult layouts, use AI-enhanced extraction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set your Gemini API key (if not in .Renviron)\n# Sys.setenv(GEMINI_API_KEY = \"your-api-key-here\")\n\n# Use AI-enhanced extraction for complex PDFs\ndoc_enhanced <- pdf2txt_auto(\n  \"complex_paper.pdf\",\n  n_columns = 2,\n  use_ai = TRUE,           # Enable AI processing\n  ai_model = \"2.0-flash\"   # Use Gemini 2.0 Flash\n)\n\n# Process large PDFs in chunks\nlarge_doc <- process_large_pdf(\n  \"large_paper.pdf\",\n  chunk_pages = 10,        # Process 10 pages at a time\n  ai_model = \"2.0-flash\"\n)\n\n# Direct AI content analysis\nresult <- gemini_content_ai(\n  docs = \"paper.pdf\",\n  prompt = \"Extract and structure all citations from this document\",\n  outputSize = \"large\"\n)\n```\n:::\n\n\n## Export Results\n\nSave your analysis results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Export citations\nwrite.csv(analysis$citations, \n          \"citations.csv\", \n          row.names = FALSE)\n\n# Export matched references\nwrite.csv(analysis$citation_references_mapping, \n          \"matched_citations.csv\", \n          row.names = FALSE)\n\n# Export word frequencies\nwrite.csv(analysis$word_frequencies, \n          \"word_frequencies.csv\", \n          row.names = FALSE)\n```\n:::\n\n\n## Processing Multiple Papers\n\nBatch process multiple documents:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# List of papers and DOIs\npapers <- c(\"paper1.pdf\", \"paper2.pdf\", \"paper3.pdf\")\ndois <- c(\"10.xxxx/1\", \"10.xxxx/2\", \"10.xxxx/3\")\n\n# Process all papers\nresults <- lapply(seq_along(papers), function(i) {\n  doc <- pdf2txt_auto(papers[i], n_columns = 2)\n  analyze_scientific_content(\n    doc, \n    doi = dois[i],\n    mailto = \"your@email.com\"\n  )\n})\n\n# Extract citation counts\ncitation_counts <- sapply(results, function(x) {\n  x$summary$citations_extracted\n})\nnames(citation_counts) <- papers\n\nprint(citation_counts)\n```\n:::\n\n\n## Next Steps\n\nNow that you're familiar with the basics, explore:\n\n- [Reference Documentation](reference/) for detailed function descriptions\n- [Tutorial](tutorial.qmd) for complete workflow examples\n- [Citation Analysis](reference/citation-analysis.qmd) for advanced citation techniques\n- [Network Visualization](reference/network-viz.qmd) for network analysis\n\n## Troubleshooting\n\n### Common Issues\n\n**PDF won't import**\n\n- Ensure the PDF is text-based, not scanned images\n- Try different `n_columns` values (1, 2, or 3)\n- Check that the file path is correct\n\n**Citations not detected**\n\n- Verify the paper uses standard citation formats\n- Check if sections are properly detected with `names(doc)`\n- Try adjusting `window_size` parameter\n\n**Low reference matching**\n\n- Provide a DOI for CrossRef integration\n- Ensure your email is valid for CrossRef API\n- Check that the References section was properly extracted\n\n**Network won't display**\n\n- Ensure there are enough citations (`min_connections`)\n- Try adjusting `max_distance` parameter\n- Check that citations were successfully extracted\n\n::: {.callout-note}\n## Need Help?\n\nIf you encounter issues not covered here, please [open an issue](https://github.com/yourusername/contentanalysis/issues) on GitHub.\n:::\n\n## Additional Resources\n\n- [Package Vignette](https://github.com/massimoaria/contentanalysis): Detailed examples\n- [GitHub Repository](https://github.com/massimoaria/contentanalysis): Source code\n- [Issue Tracker](https://github.com/massimoaria/contentanalysis/issues): Report bugs",
    "supporting": [
      "get-started_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}