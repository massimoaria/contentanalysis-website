{
  "hash": "484ab228c334326c346c5373a247b054",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Content Analysis\"\n---\n\n## Overview\n\nThe `analyze_scientific_content()` function performs comprehensive analysis of scientific documents, including citation extraction, reference matching, text analytics, and network analysis.\n\n## Main Function\n\n### analyze_scientific_content()\n\nPerform comprehensive content analysis with CrossRef integration.\n\n**Usage**\n\n```r\nanalyze_scientific_content(\n  text,\n  doi = NULL,\n  mailto = NULL,\n  window_size = 10,\n  remove_stopwords = TRUE,\n  custom_stopwords = NULL,\n  ngram_range = c(1, 3),\n  use_sections_for_citations = TRUE\n)\n```\n\n**Arguments**\n\n- `text`: Named list from `pdf2txt_auto()` containing document text\n- `doi`: Character string. DOI of the document (optional but recommended)\n- `mailto`: Character string. Email for CrossRef API access (required if using DOI)\n- `window_size`: Integer. Number of words before/after citations to extract\n- `remove_stopwords`: Logical. Remove common English stopwords\n- `custom_stopwords`: Character vector. Additional stopwords to remove\n- `ngram_range`: Numeric vector of length 2. Range of n-grams to extract (e.g., c(1,3) for unigrams to trigrams)\n- `use_sections_for_citations`: Logical. Include section information in citation analysis\n\n**Value**\n\nA list containing:\n\n- `text_analytics`: Summary statistics about the text\n- `citations`: Data frame of extracted citations\n- `citation_contexts`: Citation contexts with surrounding text\n- `citation_metrics`: Citation statistics by section\n- `citation_references_mapping`: Matched citations and references\n- `parsed_references`: Parsed reference entries\n- `word_frequencies`: Word frequency table\n- `ngrams`: N-gram frequency lists\n- `network_data`: Citation co-occurrence data\n- `section_colors`: Color mapping for sections\n- `summary`: Overall summary statistics\n\n## Basic Usage\n\n### Simple Analysis\n\nAnalyze without CrossRef integration:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(contentanalysis)\n\n# Import document\ndoc <- pdf2txt_auto(\"paper.pdf\", n_columns = 2)\n\n# Basic analysis\nanalysis <- analyze_scientific_content(\n  text = doc,\n  window_size = 10,\n  remove_stopwords = TRUE\n)\n\n# View summary\nprint(analysis$summary)\n```\n:::\n\n\n### With CrossRef Integration\n\nEnhanced analysis with automatic reference matching:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Analysis with DOI (recommended)\nanalysis <- analyze_scientific_content(\n  text = doc,\n  doi = \"10.1016/j.mlwa.2021.100094\",\n  mailto = \"your@email.com\",\n  window_size = 10,\n  remove_stopwords = TRUE,\n  ngram_range = c(1, 3)\n)\n\n# Check reference matching quality\nmatching_rate <- analysis$summary$references_matched / \n                 analysis$summary$citations_extracted\ncat(\"Matching rate:\", round(matching_rate * 100, 1), \"%\\n\")\n```\n:::\n\n\n## Understanding Results\n\n### Summary Statistics\n\nThe summary provides key metrics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanalysis$summary\n\n# Example output:\n# $total_words: 5234\n# $citations_extracted: 42\n# $narrative_citations: 18\n# $parenthetical_citations: 24\n# $references_matched: 38\n# $lexical_diversity: 0.421\n# $citation_density: 8.03 (citations per 1000 words)\n```\n:::\n\n\n### Text Analytics\n\nBasic text statistics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanalysis$text_analytics\n\n# Includes:\n# - Total words\n# - Unique words\n# - Lexical diversity\n# - Average word length\n# - Sentence count (if available)\n```\n:::\n\n\n### Citation Extraction\n\nView extracted citations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\n# All citations\nhead(analysis$citations)\n\n# Citation types\ntable(analysis$citations$citation_type)\n#  narrative parenthetical \n#        18            24\n\n# Citations by section\nanalysis$citation_metrics$section_distribution\n```\n:::\n\n\n### Citation Contexts\n\nAccess text surrounding citations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# View citation contexts\ncontexts <- analysis$citation_contexts %>%\n  select(citation_text_clean, section, words_before, words_after)\n\nhead(contexts, 3)\n\n# Example:\n# citation_text_clean    section       words_before              words_after\n# \"Breiman (2001)\"       Introduction  \"as shown by\"             \"the method provides\"\n# \"(Smith & Jones 2020)\" Methods       \"following the approach\"  \"we implemented the\"\n```\n:::\n\n\n## Advanced Usage\n\n### Custom Stopwords\n\nAdd domain-specific stopwords:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define custom stopwords\ncustom_stops <- c(\"however\", \"therefore\", \"thus\", \"moreover\", \n                  \"furthermore\", \"additionally\", \"specifically\")\n\nanalysis <- analyze_scientific_content(\n  text = doc,\n  doi = \"10.xxxx/xxxxx\",\n  mailto = \"your@email.com\",\n  custom_stopwords = custom_stops,\n  remove_stopwords = TRUE\n)\n\n# Compare with default\ntop_words_custom <- head(analysis$word_frequencies$word, 20)\n```\n:::\n\n\n### Adjusting Citation Window\n\nControl context extraction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Narrow window for focused context\nanalysis_narrow <- analyze_scientific_content(\n  text = doc,\n  window_size = 5,  # 5 words before/after\n  doi = \"10.xxxx/xxxxx\",\n  mailto = \"your@email.com\"\n)\n\n# Wide window for broader context\nanalysis_wide <- analyze_scientific_content(\n  text = doc,\n  window_size = 20,  # 20 words before/after\n  doi = \"10.xxxx/xxxxx\",\n  mailto = \"your@email.com\"\n)\n\n# Compare context lengths\nmean(nchar(analysis_narrow$citation_contexts$words_before))\nmean(nchar(analysis_wide$citation_contexts$words_before))\n```\n:::\n\n\n### N-gram Configuration\n\nExtract different n-gram ranges:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Only unigrams and bigrams\nanalysis_12 <- analyze_scientific_content(\n  text = doc,\n  ngram_range = c(1, 2)\n)\n\n# Up to 4-grams\nanalysis_14 <- analyze_scientific_content(\n  text = doc,\n  ngram_range = c(1, 4)\n)\n\n# Only bigrams and trigrams\nanalysis_23 <- analyze_scientific_content(\n  text = doc,\n  ngram_range = c(2, 3)\n)\n\n# View results\nhead(analysis_14$ngrams$`4gram`, 10)\n```\n:::\n\n\n## Working with Results\n\n### Citation Analysis Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Find citations to specific author\nbreiman_cites <- analysis$citation_references_mapping %>%\n  filter(grepl(\"Breiman\", ref_authors, ignore.case = TRUE))\n\nnrow(breiman_cites)\n\n# 2. Citations in Introduction\nintro_cites <- analysis$citations %>%\n  filter(section == \"Introduction\")\n\n# 3. Most cited references\ncite_counts <- analysis$citation_references_mapping %>%\n  count(ref_full_text, sort = TRUE)\n\nhead(cite_counts, 10)\n\n# 4. Narrative vs parenthetical by section\ncitation_types_by_section <- analysis$citations %>%\n  group_by(section, citation_type) %>%\n  summarise(count = n(), .groups = \"drop\")\n\nprint(citation_types_by_section)\n```\n:::\n\n\n### Text Analysis Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Most frequent words\ntop_20_words <- head(analysis$word_frequencies, 20)\n\n# 2. Domain-specific terms (e.g., methods)\nmethod_terms <- analysis$word_frequencies %>%\n  filter(grepl(\"model|algorithm|method|approach\", word))\n\nhead(method_terms, 10)\n\n# 3. Most common bigrams\ntop_bigrams <- head(analysis$ngrams$`2gram`, 15)\n\n# 4. Technical trigrams\ntech_trigrams <- analysis$ngrams$`3gram` %>%\n  filter(frequency > 2)\n\nhead(tech_trigrams)\n```\n:::\n\n\n### Reference Matching Quality\n\nAssess matching quality:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# View matching diagnostics\nprint_matching_diagnostics(analysis)\n\n# Confidence distribution\ntable(analysis$citation_references_mapping$match_confidence)\n\n# High confidence matches\nhigh_conf <- analysis$citation_references_mapping %>%\n  filter(match_confidence == \"high\")\n\ncat(\"High confidence matches:\", nrow(high_conf), \"\\n\")\n\n# Low confidence matches (may need review)\nlow_conf <- analysis$citation_references_mapping %>%\n  filter(match_confidence == \"low\")\n\nif (nrow(low_conf) > 0) {\n  cat(\"Low confidence matches requiring review:\\n\")\n  print(low_conf %>% select(citation_text_clean, ref_full_text))\n}\n```\n:::\n\n\n## Export and Reporting\n\n### Export Analysis Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create output directory\ndir.create(\"analysis_output\", showWarnings = FALSE)\n\n# Export main results\nwrite.csv(analysis$citations, \n          \"analysis_output/citations.csv\", \n          row.names = FALSE)\n\nwrite.csv(analysis$citation_references_mapping,\n          \"analysis_output/matched_references.csv\",\n          row.names = FALSE)\n\nwrite.csv(analysis$word_frequencies,\n          \"analysis_output/word_frequencies.csv\",\n          row.names = FALSE)\n\nwrite.csv(analysis$citation_contexts,\n          \"analysis_output/citation_contexts.csv\",\n          row.names = FALSE)\n\n# Export n-grams\nfor (n in names(analysis$ngrams)) {\n  write.csv(analysis$ngrams[[n]],\n            paste0(\"analysis_output/\", n, \".csv\"),\n            row.names = FALSE)\n}\n```\n:::\n\n\n### Generate Report\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create summary report\nreport <- list(\n  document_info = list(\n    doi = \"10.xxxx/xxxxx\",\n    total_words = analysis$summary$total_words,\n    sections = names(doc)[names(doc) != \"Full_text\"]\n  ),\n  citation_stats = analysis$summary[grep(\"citation\", names(analysis$summary))],\n  top_words = head(analysis$word_frequencies, 10),\n  top_bigrams = head(analysis$ngrams$`2gram`, 10),\n  section_citation_counts = analysis$citation_metrics$section_distribution\n)\n\n# Save as JSON\nlibrary(jsonlite)\nwrite_json(report, \"analysis_output/summary_report.json\", \n           pretty = TRUE, auto_unbox = TRUE)\n```\n:::\n\n\n## Batch Processing\n\nProcess multiple documents:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define papers to process\npapers <- data.frame(\n  file = c(\"paper1.pdf\", \"paper2.pdf\", \"paper3.pdf\"),\n  doi = c(\"10.1000/1\", \"10.1000/2\", \"10.1000/3\"),\n  stringsAsFactors = FALSE\n)\n\n# Process all\nresults <- list()\nfor (i in 1:nrow(papers)) {\n  cat(\"Processing:\", papers$file[i], \"\\n\")\n  \n  doc <- pdf2txt_auto(papers$file[i], n_columns = 2)\n  results[[i]] <- analyze_scientific_content(\n    text = doc,\n    doi = papers$doi[i],\n    mailto = \"your@email.com\"\n  )\n  \n  Sys.sleep(1)  # Be polite to CrossRef API\n}\n\nnames(results) <- papers$file\n\n# Compare results\ncomparison <- data.frame(\n  file = papers$file,\n  words = sapply(results, function(r) r$summary$total_words),\n  citations = sapply(results, function(r) r$summary$citations_extracted),\n  matched = sapply(results, function(r) r$summary$references_matched),\n  diversity = sapply(results, function(r) r$summary$lexical_diversity)\n)\n\nprint(comparison)\n```\n:::\n\n\n## Tips and Best Practices\n\n::: {.callout-tip}\n## DOI and Email\n\nAlways provide DOI and email when possible:\n\n- Enables automatic CrossRef reference matching\n- Dramatically improves citation-reference linking\n- Provides metadata about the document\n- Email should be valid (CrossRef policy)\n:::\n\n::: {.callout-tip}\n## Window Size\n\nChoose window size based on your needs:\n\n- **Small (5-7)**: Focused analysis, immediate context\n- **Medium (8-12)**: Balanced approach (recommended)\n- **Large (15-20)**: Broader context, sentence-level analysis\n:::\n\n::: {.callout-warning}\n## CrossRef API\n\nBe respectful of CrossRef API:\n\n- Use valid email in `mailto`\n- Add delays between requests in batch processing\n- Consider rate limits for large-scale analysis\n- Cache results when possible\n:::\n\n## See Also\n\n- [Citation Analysis](citation-analysis.qmd): Deep dive into citation features\n- [Network Visualization](network-viz.qmd): Create citation networks\n- [Text Analysis](text-analysis.qmd): Advanced text analytics\n- [Tutorial](../tutorial.qmd): Complete workflow examples",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}