{
  "hash": "cdb2c0bc6de9acf7ffd4bedd327ed81c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Text Analysis Functions\"\n---\n\n## Overview\n\nThe contentanalysis package provides comprehensive text analysis capabilities including word frequency analysis, n-gram extraction, and word distribution tracking across document sections.\n\n## Word Frequency Analysis\n\n### Automatic Extraction\n\nWord frequencies are automatically calculated during content analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(contentanalysis)\nlibrary(dplyr)\n\n# Analyze document\ndoc <- pdf2txt_auto(\"paper.pdf\", n_columns = 2)\nanalysis <- analyze_scientific_content(\n  text = doc,\n  remove_stopwords = TRUE\n)\n\n# View word frequencies\nhead(analysis$word_frequencies, 20)\n```\n:::\n\n\n### Custom Stopwords\n\nAdd domain-specific stopwords:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define custom stopwords\ncustom_stops <- c(\"however\", \"therefore\", \"thus\", \"moreover\",\n                  \"furthermore\", \"additionally\", \"specifically\",\n                  \"particularly\", \"generally\", \"typically\")\n\nanalysis <- analyze_scientific_content(\n  text = doc,\n  custom_stopwords = custom_stops,\n  remove_stopwords = TRUE\n)\n\n# Compare top words\nhead(analysis$word_frequencies, 20)\n```\n:::\n\n\n### Word Frequency Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Top 50 words\ntop_50 <- head(analysis$word_frequencies, 50)\n\n# Visualize\nlibrary(ggplot2)\ntop_20 <- head(analysis$word_frequencies, 20)\n\nggplot(top_20, aes(x = reorder(word, frequency), y = frequency)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 20 Most Frequent Words\",\n       x = \"Word\", y = \"Frequency\") +\n  theme_minimal()\n\n# Word cloud\nlibrary(wordcloud)\nwordcloud(words = analysis$word_frequencies$word,\n          freq = analysis$word_frequencies$frequency,\n          max.words = 100,\n          colors = brewer.pal(8, \"Dark2\"))\n```\n:::\n\n\n## N-gram Analysis\n\n### Extracting N-grams\n\nN-grams are automatically extracted:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Configure n-gram range during analysis\nanalysis <- analyze_scientific_content(\n  text = doc,\n  ngram_range = c(1, 3),  # Unigrams to trigrams\n  remove_stopwords = TRUE\n)\n\n# Access n-grams\nnames(analysis$ngrams)\n# [1] \"1gram\" \"2gram\" \"3gram\"\n\n# View bigrams\nhead(analysis$ngrams$`2gram`, 20)\n\n# View trigrams\nhead(analysis$ngrams$`3gram`, 20)\n```\n:::\n\n\n### N-gram Configurations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Only bigrams and trigrams\nanalysis_23 <- analyze_scientific_content(\n  text = doc,\n  ngram_range = c(2, 3)\n)\n\n# Up to 4-grams\nanalysis_14 <- analyze_scientific_content(\n  text = doc,\n  ngram_range = c(1, 4)\n)\n\n# Only bigrams\nanalysis_2 <- analyze_scientific_content(\n  text = doc,\n  ngram_range = c(2, 2)\n)\n\n# Compare\ncat(\"Bigrams found:\", nrow(analysis_2$ngrams$`2gram`), \"\\n\")\ncat(\"Trigrams found:\", nrow(analysis_23$ngrams$`3gram`), \"\\n\")\ncat(\"4-grams found:\", nrow(analysis_14$ngrams$`4gram`), \"\\n\")\n```\n:::\n\n\n### Analyzing N-grams\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most frequent bigrams\ntop_bigrams <- head(analysis$ngrams$`2gram`, 20)\n\n# Visualize\nggplot(top_bigrams, aes(x = reorder(ngram, frequency), y = frequency)) +\n  geom_col(fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Top 20 Most Frequent Bigrams\",\n       x = \"Bigram\", y = \"Frequency\") +\n  theme_minimal()\n\n# Filter by topic\nmethod_bigrams <- analysis$ngrams$`2gram` %>%\n  filter(grepl(\"model|method|algorithm|approach\", ngram, ignore.case = TRUE))\n\ncat(\"Method-related bigrams:\", nrow(method_bigrams), \"\\n\")\nhead(method_bigrams, 10)\n```\n:::\n\n\n## Word Distribution Analysis\n\n### calculate_word_distribution()\n\nTrack how specific terms are distributed across the document.\n\n**Usage**\n\n```r\ncalculate_word_distribution(\n  text,\n  selected_words,\n  use_sections = TRUE,\n  n_segments = 10,\n  normalize = TRUE\n)\n```\n\n**Arguments**\n\n- `text`: Named list from `pdf2txt_auto()`\n- `selected_words`: Character vector of terms to track\n- `use_sections`: Logical. Use document sections (TRUE) or equal segments (FALSE)\n- `n_segments`: Number of segments if `use_sections = FALSE`\n- `normalize`: Logical. Normalize counts as percentages\n\n### Section-Based Distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define terms of interest\nterms <- c(\"machine learning\", \"random forest\", \n           \"accuracy\", \"classification\", \"tree\")\n\n# Calculate distribution by section\ndist <- calculate_word_distribution(\n  text = doc,\n  selected_words = terms,\n  use_sections = TRUE,\n  normalize = TRUE\n)\n\n# View results\ndist %>%\n  select(segment_name, word, count, percentage) %>%\n  arrange(segment_name, desc(percentage))\n\n# Summary statistics\ndist %>%\n  group_by(word) %>%\n  summarise(\n    total_count = sum(count),\n    max_section = segment_name[which.max(percentage)],\n    max_percentage = max(percentage)\n  ) %>%\n  arrange(desc(total_count))\n```\n:::\n\n\n### Segment-Based Distribution\n\nFor uniform analysis across document:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Divide into equal segments\ndist_segments <- calculate_word_distribution(\n  text = doc,\n  selected_words = terms,\n  use_sections = FALSE,\n  n_segments = 20\n)\n\n# Track term evolution\nterm_evolution <- dist_segments %>%\n  filter(word == \"machine learning\") %>%\n  select(segment_name, segment_index, percentage)\n\nprint(term_evolution)\n```\n:::\n\n\n### Comparing Terms\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare usage patterns\nlibrary(tidyr)\n\ncomparison <- dist %>%\n  select(segment_name, word, percentage) %>%\n  pivot_wider(names_from = word, values_from = percentage)\n\nprint(comparison)\n\n# Find section with highest term density\ndensity_by_section <- dist %>%\n  group_by(segment_name) %>%\n  summarise(total_percentage = sum(percentage)) %>%\n  arrange(desc(total_percentage))\n\ncat(\"Section with highest term density:\\n\")\nprint(head(density_by_section))\n```\n:::\n\n\n## Visualization Functions\n\n### plot_word_distribution()\n\nCreate interactive visualizations of word distributions.\n\n**Usage**\n\n```r\nplot_word_distribution(\n  distribution_data,\n  plot_type = \"line\",\n  show_points = TRUE,\n  smooth = FALSE,\n  color_palette = NULL\n)\n```\n\n**Arguments**\n\n- `distribution_data`: Output from `calculate_word_distribution()`\n- `plot_type`: \"line\", \"bar\", or \"area\"\n- `show_points`: Logical. Show data points on line plots\n- `smooth`: Logical. Add smoothed trend line\n- `color_palette`: Custom color palette\n\n### Line Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Basic line plot\nplot_word_distribution(\n  dist,\n  plot_type = \"line\",\n  show_points = TRUE\n)\n\n# With smoothing\nplot_word_distribution(\n  dist,\n  plot_type = \"line\",\n  show_points = TRUE,\n  smooth = TRUE\n)\n\n# Segment-based with smooth trends\nplot_word_distribution(\n  dist_segments,\n  plot_type = \"line\",\n  smooth = TRUE\n)\n```\n:::\n\n\n### Bar Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bar plot by section\nplot_word_distribution(\n  dist,\n  plot_type = \"bar\"\n)\n\n# Custom colors\ncustom_colors <- c(\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\")\n\nplot_word_distribution(\n  dist,\n  plot_type = \"bar\",\n  color_palette = custom_colors\n)\n```\n:::\n\n\n### Area Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Area plot\nplot_word_distribution(\n  dist,\n  plot_type = \"area\"\n)\n\n# Stacked area for segment analysis\nplot_word_distribution(\n  dist_segments,\n  plot_type = \"area\"\n)\n```\n:::\n\n\n## Advanced Text Analysis\n\n### Lexical Diversity\n\nMeasure vocabulary richness:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# From analysis summary\nanalysis$summary$lexical_diversity\n\n# Calculate manually\nwords <- tolower(unlist(strsplit(doc$Full_text, \"\\\\s+\")))\nwords_clean <- words[!words %in% stopwords::stopwords(\"en\")]\n\nlexical_div <- length(unique(words_clean)) / length(words_clean)\ncat(\"Lexical diversity:\", round(lexical_div, 3), \"\\n\")\n\n# By section\nsection_diversity <- sapply(doc[names(doc) != \"Full_text\"], function(section_text) {\n  words <- tolower(unlist(strsplit(section_text, \"\\\\s+\")))\n  words_clean <- words[!words %in% stopwords::stopwords(\"en\")]\n  length(unique(words_clean)) / length(words_clean)\n})\n\nprint(sort(section_diversity, decreasing = TRUE))\n```\n:::\n\n\n### Term Co-occurrence\n\nFind terms that appear together:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract bigrams with specific terms\nml_bigrams <- analysis$ngrams$`2gram` %>%\n  filter(grepl(\"machine|learning\", ngram, ignore.case = TRUE))\n\nprint(ml_bigrams)\n\n# Create co-occurrence matrix\nlibrary(quanteda)\nlibrary(quanteda.textstats)\n\ntokens <- tokens(doc$Full_text, remove_punct = TRUE)\ntokens_lower <- tokens_tolower(tokens)\n\n# Co-occurrence within 5-word window\nfcm <- fcm(tokens_lower, context = \"window\", window = 5)\n\n# Top co-occurrences with \"machine\"\ntopfeatures(fcm[\"machine\", ], 20)\n```\n:::\n\n\n### Keyword Extraction\n\nIdentify key terms using TF-IDF:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple TF-IDF approach\nlibrary(tidytext)\n\n# Prepare data\ntext_df <- data.frame(\n  section = names(doc)[names(doc) != \"Full_text\"],\n  text = unlist(doc[names(doc) != \"Full_text\"])\n)\n\n# Calculate TF-IDF\nwords_df <- text_df %>%\n  unnest_tokens(word, text) %>%\n  count(section, word) %>%\n  bind_tf_idf(word, section, n)\n\n# Top TF-IDF words per section\ntop_tfidf <- words_df %>%\n  group_by(section) %>%\n  slice_max(tf_idf, n = 10) %>%\n  ungroup()\n\n# Visualize\nggplot(top_tfidf, aes(x = reorder_within(word, tf_idf, section), \n                      y = tf_idf, fill = section)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~section, scales = \"free\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(title = \"Top Terms by Section (TF-IDF)\",\n       x = NULL, y = \"TF-IDF\") +\n  theme_minimal()\n```\n:::\n\n\n### Sentiment Analysis\n\nBasic sentiment scoring:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\n\n# Get sentiment lexicon\nsentiments <- get_sentiments(\"bing\")\n\n# Calculate sentiment by section\nsection_sentiment <- text_df %>%\n  unnest_tokens(word, text) %>%\n  inner_join(sentiments, by = \"word\") %>%\n  count(section, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%\n  mutate(\n    sentiment_score = positive - negative,\n    sentiment_ratio = positive / (positive + negative)\n  )\n\nprint(section_sentiment)\n\n# Visualize\nggplot(section_sentiment, aes(x = section, y = sentiment_score, fill = section)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Sentiment Score by Section\",\n       x = \"Section\", y = \"Sentiment Score (Positive - Negative)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n:::\n\n\n## Export Text Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create export directory\ndir.create(\"text_analysis\", showWarnings = FALSE)\n\n# 1. Word frequencies\nwrite.csv(analysis$word_frequencies,\n          \"text_analysis/word_frequencies.csv\",\n          row.names = FALSE)\n\n# 2. N-grams\nfor (n in names(analysis$ngrams)) {\n  write.csv(analysis$ngrams[[n]],\n            paste0(\"text_analysis/\", n, \".csv\"),\n            row.names = FALSE)\n}\n\n# 3. Word distribution\nwrite.csv(dist,\n          \"text_analysis/word_distribution.csv\",\n          row.names = FALSE)\n\n# 4. Summary statistics\nsummary_stats <- data.frame(\n  metric = c(\"total_words\", \"unique_words\", \"lexical_diversity\"),\n  value = c(\n    analysis$summary$total_words,\n    nrow(analysis$word_frequencies),\n    analysis$summary$lexical_diversity\n  )\n)\n\nwrite.csv(summary_stats,\n          \"text_analysis/summary_statistics.csv\",\n          row.names = FALSE)\n```\n:::\n\n\n## Tips and Best Practices\n\n::: {.callout-tip}\n## Stopword Management\n\n- Use `remove_stopwords = TRUE` for cleaner analysis\n- Add domain-specific terms to `custom_stopwords`\n- Keep some function words for n-gram analysis\n- Review top words to identify missed stopwords\n:::\n\n::: {.callout-tip}\n## N-gram Selection\n\n- Bigrams capture common phrases\n- Trigrams identify technical terms\n- 4-grams useful for specific methodologies\n- Balance between detail and interpretability\n:::\n\n::: {.callout-tip}\n## Word Distribution\n\n- Use sections for structural analysis\n- Use segments for temporal evolution\n- Track 3-5 key terms for clarity\n- Normalize for fair comparison\n:::\n\n## See Also\n\n- [Content Analysis](content-analysis.qmd): Main analysis function\n- [Readability Metrics](readability.qmd): Assess text complexity\n- [Tutorial](../tutorial.qmd): Complete workflow examples",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}